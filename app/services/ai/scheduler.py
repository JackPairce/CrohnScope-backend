"""
Scheduler module for AI training tasks

This module provides scheduling functionality to periodically check for
training conditions and trigger model training when needed using a separate process.
"""

# Generated by Copilot
import threading
import subprocess
import os
import time
import json
import logging
from pathlib import Path
from app.services.ai.train import RETRAINING_THRESHOLD
from app.types.training import ProcessInfo, TrainingStatusEnum
from typing import Union

logger = logging.getLogger(__name__)

# Path to store training process info
PROCESS_INFO_FILE = "data/models/training_process.json"


class TrainingScheduler:
    """Scheduler to periodically check and trigger training when needed in a separate process."""

    def __init__(self, check_interval=60):
        """
        Initialize the training scheduler.

        Args:
            check_interval: Time in seconds between checks for training conditions
        """
        self.check_interval = check_interval  # Check every minute by default
        self._stop_event = threading.Event()
        self._thread = None
        self._running = False
        self._process = None
        self._ensure_paths_exist()

    def _ensure_paths_exist(self):
        """Ensure all necessary directories exist."""
        os.makedirs(os.path.dirname(PROCESS_INFO_FILE), exist_ok=True)

    def _is_training_running(self):
        """Check if a training process is currently running."""
        process_info = self._read_process_info()

        # If no PID, not running
        if not process_info.pid:
            return False

        try:
            # Check if process is running (no error means it exists)
            os.kill(process_info.pid, 0)

            # Check if it's actually our process and not stuck
            current_time = time.time()
            # If process is running for more than 24 hours, assume it's stuck
            if current_time - process_info.start_time > 86400:  # 24 hours
                logger.warning(
                    f"Training process {process_info.pid} has been running for > 24 hours, considering it stuck"
                )
                return False

            # Process exists and is not stuck
            return True
        except OSError:
            # Process doesn't exist
            return False

    def _start_training_process(self):
        """Start a separate process for training."""
        if self._is_training_running():
            logger.info("Training already in progress, skipping")
            return

        # Get absolute path to the training script
        script_dir = Path(__file__).parent.absolute()
        train_script = script_dir / "train_worker.py"

        # Check if script exists, create it if not
        if not train_script.exists():
            self._create_training_script(train_script)

        try:
            # Start training in a separate process
            process = subprocess.Popen(
                ["python", str(train_script)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
            )

            # Store process info
            process_info = ProcessInfo(
                pid=process.pid,
                status=TrainingStatusEnum.RUNNING,
                start_time=time.time(),
                last_update=time.time(),
            )

            if not self._write_process_info(process_info):
                logger.error("Failed to write process info, but process was started")

            logger.info(f"Started training process with PID {process.pid}")
            self._process = process

        except Exception as e:
            logger.error(f"Failed to start training process: {e}")

    def _create_training_script(self, script_path):
        """Create the training worker script if it doesn't exist."""
        logger.info(f"Creating training worker script at {script_path}")
        content = '''"""
Training worker script for running model training in a separate process.
"""
import os
import sys
import time
import json
import logging
from pathlib import Path
from enum import Enum

# Add the project root to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("data/models/training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("train_worker")

# Import the training function and types
from app.services.ai.train import start_training_if_needed
from app.types.training import ProcessInfo, TrainingStatusEnum

# Process info file path
PROCESS_INFO_FILE = "data/models/training_process.json"

def update_status(status, error=None):
    """Update the status of the training process using ProcessInfo model."""
    try:
        # Load current process info
        if os.path.exists(PROCESS_INFO_FILE):
            with open(PROCESS_INFO_FILE, 'r') as f:
                data = json.load(f)

            # Create ProcessInfo instance, preserving existing data
            process_info = ProcessInfo(**data)
        else:
            logger.error("Process info file not found, cannot update status")
            return False

        # Update status and error if provided
        process_info.status = TrainingStatusEnum(status) if isinstance(status, str) else status
        process_info.last_update = time.time()

        if error is not None:
            process_info.error = str(error)

        # Write back to file atomically
        temp_file = f"{PROCESS_INFO_FILE}.tmp"
        with open(temp_file, 'w') as f:
            json.dump(process_info.dict(), f, indent=2)

        os.replace(temp_file, PROCESS_INFO_FILE)
        return True
    except Exception as e:
        logger.error(f"Failed to update process status: {e}")
        return False

if __name__ == "__main__":
    logger.info("Training worker started")
    try:
        update_status(TrainingStatusEnum.RUNNING)
        # Run the actual training
        result = start_training_if_needed()
        if result:
            update_status(TrainingStatusEnum.COMPLETED)
            logger.info("Training completed successfully")
        else:
            update_status(TrainingStatusEnum.SKIPPED)
            logger.info("Training conditions not met, skipped")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        update_status(TrainingStatusEnum.FAILED, error=str(e))
    finally:
        # Ensure we don't leave the process in a running state
        try:
            with open(PROCESS_INFO_FILE, 'r') as f:
                process_info = json.load(f)
            if process_info.get("status") == TrainingStatusEnum.RUNNING:
                update_status(TrainingStatusEnum.FAILED, error="Process terminated unexpectedly")
        except Exception as e:
            logger.error(f"Error in cleanup: {e}")

        logger.info("Training worker exiting")
'''
        with open(script_path, "w") as f:
            f.write(content)

    def _run(self):
        """Main scheduler loop that checks for training conditions."""
        while not self._stop_event.is_set():
            try:
                # Check if training should be started
                needs_training = self._check_training_needed()
                if needs_training and not self._is_training_running():
                    self._start_training_process()

                # Check status of current process
                self._check_process_status()
            except Exception as e:
                logger.error(f"Error in training scheduler: {e}")

            # Wait for the next check interval or until stop is called
            self._stop_event.wait(self.check_interval)

    def _check_training_needed(self):
        """Check if training is needed based on database changes."""
        # This is a simplified check - in production, you'd want to query
        # your database to see if there are enough new records since last training
        return True  # For now, we'll rely on start_training_if_needed() to decide

    def _check_process_status(self):
        """Check the status of the current training process."""
        if not self._process:
            return

        # Check if process is still running
        returncode = self._process.poll()
        if returncode is not None:
            # Process has finished
            stdout, stderr = self._process.communicate()
            if returncode == 0:
                logger.info("Training process completed successfully")
            else:
                logger.error(
                    f"Training process failed with code {returncode}: {stderr}"
                )

            self._process = None

    def start(self):
        """Start the scheduler in a background thread."""
        if self._thread is not None and self._thread.is_alive():
            # Already running
            return False

        self._stop_event.clear()
        self._thread = threading.Thread(target=self._run)
        self._thread.daemon = True
        self._thread.start()
        self._running = True
        logger.info(
            f"Training scheduler started with check interval: {self.check_interval}s"
        )
        return True

    def stop(self):
        """Stop the scheduler."""
        if self._thread is None or not self._thread.is_alive():
            # Not running
            return False

        self._stop_event.set()
        self._thread.join(timeout=5.0)
        self._running = False

        # Try to terminate any running process
        if self._process and self._process.poll() is None:
            self._process.terminate()
            self._process.wait(timeout=5.0)

        logger.info("Training scheduler stopped")
        return True

    @property
    def is_running(self):
        """Check if the scheduler is currently running."""
        return self._running and self._thread is not None and self._thread.is_alive()

    def get_training_status(self) -> dict:
        """Get the current status of training with type validation.

        Returns:
            dict: Status information including process ID, start time, and current status
        """
        process_info = self._read_process_info()

        # Check if process is still running
        if process_info.pid:
            try:
                os.kill(process_info.pid, 0)  # Check if process exists
            except OSError:
                # Process doesn't exist anymore, update status
                process_info.status = TrainingStatusEnum.NOT_RUNNING
                process_info.pid = None
                # Update the file with new status
                self._write_process_info(process_info)

        # Calculate elapsed time if available
        if process_info.start_time > 0:
            elapsed = time.time() - process_info.start_time
            result_dict = process_info.dict()
            result_dict["elapsed_seconds"] = elapsed
            return result_dict

        return process_info.dict()

    # Generated by Copilot
    def _write_process_info(self, process_info: Union[ProcessInfo, dict]) -> bool:
        """Write process information to the status file with type safety checks.

        Args:
            process_info: ProcessInfo model or dictionary containing process information

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Convert dict to ProcessInfo if needed
            if isinstance(process_info, dict):
                try:
                    # Convert status string to enum if needed
                    if "status" in process_info and isinstance(
                        process_info["status"], str
                    ):
                        process_info["status"] = TrainingStatusEnum(
                            process_info["status"]
                        )
                    model = ProcessInfo(**process_info)
                except Exception as e:
                    logger.error(f"Failed to convert dict to ProcessInfo: {e}")
                    return False
            else:
                model = process_info

            # Ensure directory exists
            self._ensure_paths_exist()

            # Write atomically by using a temporary file
            temp_file = f"{PROCESS_INFO_FILE}.tmp"
            with open(temp_file, "w") as f:
                json.dump(model.dict(), f, indent=2)

            # Atomic rename to avoid partial writes
            os.replace(temp_file, PROCESS_INFO_FILE)
            return True
        except Exception as e:
            logger.error(f"Failed to write process info: {e}")
            return False

    # Generated by Copilot
    def _read_process_info(self) -> ProcessInfo:
        """Read process information from the status file with type validation.

        Returns:
            ProcessInfo: Process information model or default values if file doesn't exist or is invalid
        """
        if not os.path.exists(PROCESS_INFO_FILE):
            return ProcessInfo()  # Return default model

        try:
            with open(PROCESS_INFO_FILE, "r") as f:
                data = json.load(f)

            # Try to parse as ProcessInfo with validation
            try:
                # Convert status string to enum if needed
                if "status" in data and isinstance(data["status"], str):
                    try:
                        data["status"] = TrainingStatusEnum(data["status"])
                    except ValueError:
                        logger.warning(f"Invalid status value: {data['status']}")
                        data["status"] = TrainingStatusEnum.UNKNOWN

                return ProcessInfo(**data)
            except Exception as e:
                logger.error(f"Error parsing process info, using defaults: {e}")
                return ProcessInfo()

        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"Error reading process info: {e}")
            return ProcessInfo()


# Create a singleton scheduler instance
scheduler = TrainingScheduler()
