"""
Scheduler module for AI training tasks

This module provides scheduling functionality to periodically check for
training conditions and trigger model training when needed using multiprocessing.
"""

# Generated by Copilot
import threading
import multiprocessing
import os
import time
import json
import logging
import signal
from pathlib import Path
from app.services.ai.train import RETRAINING_THRESHOLD, start_training_if_needed
from app.types.training import ProcessInfo, TrainingStatusEnum
from typing import Union, Optional

logger = logging.getLogger(__name__)

# Path to store training process info
PROCESS_INFO_FILE = "data/models/training_process.json"


# Function to run in a separate process
def training_process_function(process_info_file=PROCESS_INFO_FILE):
    """
    Function to be run in a separate process to handle model training.
    
    Args:
        process_info_file: Path to the file where process info is stored
    """
    # Configure logging for the training process
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("data/models/training.log"),
            logging.StreamHandler()
        ]
    )
    process_logger = logging.getLogger("training_process")
    
    # Helper function to update the status file
    def update_status(status, error=None):
        """Update the status of the training process."""
        try:
            # Load current process info
            if os.path.exists(process_info_file):
                with open(process_info_file, 'r') as f:
                    data = json.load(f)
                
                # Create ProcessInfo instance, preserving existing data
                process_info = ProcessInfo(**data)
            else:
                process_logger.error("Process info file not found, cannot update status")
                return False
            
            # Update status and error if provided
            process_info.status = TrainingStatusEnum(status) if isinstance(status, str) else status
            process_info.last_update = time.time()
            
            if error is not None:
                process_info.error = str(error)
                
            # Write back to file atomically
            temp_file = f"{process_info_file}.tmp"
            with open(temp_file, 'w') as f:
                json.dump(process_info.dict(), f, indent=2)
                
            os.replace(temp_file, process_info_file)
            return True
        except Exception as e:
            process_logger.error(f"Failed to update process status: {e}")
            return False
    
    # Main training execution
    process_logger.info("Training process started")
    try:
        update_status(TrainingStatusEnum.RUNNING)
        
        # Run the actual training
        result = start_training_if_needed()
        
        if result:
            update_status(TrainingStatusEnum.COMPLETED)
            process_logger.info("Training completed successfully")
        else:
            update_status(TrainingStatusEnum.SKIPPED)
            process_logger.info("Training conditions not met, skipped")
    except Exception as e:
        process_logger.error(f"Training failed: {e}")
        update_status(TrainingStatusEnum.FAILED, error=str(e))
    finally:
        # Ensure we don't leave the process in a running state
        try:
            with open(process_info_file, 'r') as f:
                data = json.load(f)
                
            if data.get("status") == TrainingStatusEnum.RUNNING.value:
                update_status(TrainingStatusEnum.FAILED, error="Process terminated unexpectedly")
        except Exception as e:
            process_logger.error(f"Error in cleanup: {e}")
            
        process_logger.info("Training process exiting")


# Function to run in a separate process
def training_process_function(process_info_file=PROCESS_INFO_FILE):
    """
    Function to be run in a separate process to handle model training.
    
    Args:
        process_info_file: Path to the file where process info is stored
    """
    # Configure logging for the training process
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("data/models/training.log"),
            logging.StreamHandler()
        ]
    )
    process_logger = logging.getLogger("training_process")
    
    # Helper function to update the status file
    def update_status(status, error=None):
        """Update the status of the training process."""
        try:
            # Load current process info
            if os.path.exists(process_info_file):
                with open(process_info_file, 'r') as f:
                    data = json.load(f)
                
                # Create ProcessInfo instance, preserving existing data
                process_info = ProcessInfo(**data)
            else:
                process_logger.error("Process info file not found, cannot update status")
                return False
            
            # Update status and error if provided
            process_info.status = TrainingStatusEnum(status) if isinstance(status, str) else status
            process_info.last_update = time.time()
            
            if error is not None:
                process_info.error = str(error)
                
            # Write back to file atomically
            temp_file = f"{process_info_file}.tmp"
            with open(temp_file, 'w') as f:
                json.dump(process_info.dict(), f, indent=2)
                
            os.replace(temp_file, process_info_file)
            return True
        except Exception as e:
            process_logger.error(f"Failed to update process status: {e}")
            return False
    
    # Main training execution
    process_logger.info("Training process started")
    try:
        update_status(TrainingStatusEnum.RUNNING)
        
        # Run the actual training
        result = start_training_if_needed()
        
        if result:
            update_status(TrainingStatusEnum.COMPLETED)
            process_logger.info("Training completed successfully")
        else:
            update_status(TrainingStatusEnum.SKIPPED)
            process_logger.info("Training conditions not met, skipped")
    except Exception as e:
        process_logger.error(f"Training failed: {e}")
        update_status(TrainingStatusEnum.FAILED, error=str(e))
    finally:
        # Ensure we don't leave the process in a running state
        try:
            with open(process_info_file, 'r') as f:
                data = json.load(f)
                
            if data.get("status") == TrainingStatusEnum.RUNNING.value:
                update_status(TrainingStatusEnum.FAILED, error="Process terminated unexpectedly")
        except Exception as e:
            process_logger.error(f"Error in cleanup: {e}")
            
        process_logger.info("Training process exiting")


class TrainingScheduler:
    """Scheduler to periodically check and trigger training when needed using multiprocessing."""

    def __init__(self, check_interval=60):
        """
        Initialize the training scheduler.

        Args:
            check_interval: Time in seconds between checks for training conditions
        """
        self.check_interval = check_interval  # Check every minute by default
        self._stop_event = threading.Event()
        self._thread = None
        self._running = False
        self._process = None
        self._ensure_paths_exist()

    def _ensure_paths_exist(self):
        """Ensure all necessary directories exist."""
        os.makedirs(os.path.dirname(PROCESS_INFO_FILE), exist_ok=True)

    def _is_training_running(self) -> bool:
        """Check if a training process is currently running.
        
        Returns:
            bool: True if a training process is running, False otherwise
        """
        process_info = self._read_process_info()

        # If no PID, not running
        if not process_info.pid:
            return False

        try:
            # Check if process is running (no error means it exists)
            os.kill(process_info.pid, 0)

            # Check if it's actually our process and not stuck
            current_time = time.time()
            # If process is running for more than 24 hours, assume it's stuck
            if current_time - process_info.start_time > 86400:  # 24 hours
                logger.warning(
                    f"Training process {process_info.pid} has been running for > 24 hours, considering it stuck"
                )
                return False

            # Process exists and is not stuck
            return True
        except OSError:
            # Process doesn't exist
            return False

    def _start_training_process(self):
        """Start a separate process for training using multiprocessing."""
        if self._is_training_running():
            logger.info("Training already in progress, skipping")
            return

        try:
            # Create and start a new process using multiprocessing
            process = multiprocessing.Process(
                target=training_process_function, 
                name="CrohnScope-Training"
            )
            
            # Start the process
            process.daemon = True  # Set as daemon so it terminates when main process ends
            process.start()

            # Store process info
            process_info = ProcessInfo(
                pid=process.pid,
                status=TrainingStatusEnum.RUNNING,
                start_time=time.time(),
                last_update=time.time(),
            )

            if not self._write_process_info(process_info):
                logger.error("Failed to write process info, but process was started")

            logger.info(f"Started training process with PID {process.pid}")
            self._process = process

        except Exception as e:
            logger.error(f"Failed to start training process: {e}")

    def _run(self):
        """Main scheduler loop that checks for training conditions."""
        while not self._stop_event.is_set():
            try:
                # Check if training should be started
                needs_training = self._check_training_needed()
                if needs_training and not self._is_training_running():
                    self._start_training_process()

                # Check status of current process
                self._check_process_status()
            except Exception as e:
                logger.error(f"Error in training scheduler: {e}")

            # Wait for the next check interval or until stop is called
            self._stop_event.wait(self.check_interval)

    def _check_training_needed(self):
        """Check if training is needed based on database changes."""
        # This is a simplified check - in production, you'd want to query
        # your database to see if there are enough new records since last training
        return True  # For now, we'll rely on start_training_if_needed() to decide

    def _check_process_status(self):
        """Check the status of the current training process."""
        if not self._process:
            return

        # Check if process is still running
        if not self._process.is_alive():
            # Process has finished
            exit_code = self._process.exitcode
            if exit_code == 0:
                logger.info("Training process completed successfully")
            else:
                logger.error(f"Training process failed with exit code {exit_code}")

            self._process = None

    def start(self):
        """Start the scheduler in a background thread."""
        if self._thread is not None and self._thread.is_alive():
            # Already running
            return False

        self._stop_event.clear()
        self._thread = threading.Thread(target=self._run)
        self._thread.daemon = True
        self._thread.start()
        self._running = True
        logger.info(
            f"Training scheduler started with check interval: {self.check_interval}s"
        )
        return True

    def stop(self):
        """Stop the scheduler."""
        if self._thread is None or not self._thread.is_alive():
            # Not running
            return False

        self._stop_event.set()
        self._thread.join(timeout=5.0)
        self._running = False

        # Try to terminate any running process
        if self._process and self._process.is_alive():
            self._process.terminate()
            # Give it a moment to terminate
            self._process.join(timeout=5.0)
            
            # If it's still alive, try to kill it (more forceful)
            if self._process.is_alive():
                logger.warning("Training process did not terminate gracefully, forcing termination")
                try:
                    # On Unix systems, we can send SIGKILL
                    os.kill(self._process.pid, signal.SIGKILL)
                except Exception as e:
                    logger.error(f"Failed to forcefully terminate process: {e}")

        logger.info("Training scheduler stopped")
        return True

    @property
    def is_running(self):
        """Check if the scheduler is currently running."""
        return self._running and self._thread is not None and self._thread.is_alive()

    def get_training_status(self) -> dict:
        """Get the current status of training with type validation.

        Returns:
            dict: Status information including process ID, start time, and current status
        """
        process_info = self._read_process_info()

        # Check if process is still running
        if process_info.pid:
            try:
                os.kill(process_info.pid, 0)  # Check if process exists
            except OSError:
                # Process doesn't exist anymore, update status
                process_info.status = TrainingStatusEnum.NOT_RUNNING
                process_info.pid = None
                # Update the file with new status
                self._write_process_info(process_info)

        # Calculate elapsed time if available
        if process_info.start_time > 0:
            elapsed = time.time() - process_info.start_time
            result_dict = process_info.dict()
            result_dict["elapsed_seconds"] = elapsed
            return result_dict

        return process_info.dict()

    # Generated by Copilot
    def _write_process_info(self, process_info: Union[ProcessInfo, dict]) -> bool:
        """Write process information to the status file with type safety checks.

        Args:
            process_info: ProcessInfo model or dictionary containing process information

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Convert dict to ProcessInfo if needed
            if isinstance(process_info, dict):
                try:
                    # Convert status string to enum if needed
                    if "status" in process_info and isinstance(
                        process_info["status"], str
                    ):
                        process_info["status"] = TrainingStatusEnum(
                            process_info["status"]
                        )
                    model = ProcessInfo(**process_info)
                except Exception as e:
                    logger.error(f"Failed to convert dict to ProcessInfo: {e}")
                    return False
            else:
                model = process_info

            # Ensure directory exists
            self._ensure_paths_exist()

            # Write atomically by using a temporary file
            temp_file = f"{PROCESS_INFO_FILE}.tmp"
            with open(temp_file, "w") as f:
                json.dump(model.dict(), f, indent=2)

            # Atomic rename to avoid partial writes
            os.replace(temp_file, PROCESS_INFO_FILE)
            return True
        except Exception as e:
            logger.error(f"Failed to write process info: {e}")
            return False

    # Generated by Copilot
    def _read_process_info(self) -> ProcessInfo:
        """Read process information from the status file with type validation.

        Returns:
            ProcessInfo: Process information model or default values if file doesn't exist or is invalid
        """
        if not os.path.exists(PROCESS_INFO_FILE):
            return ProcessInfo()  # Return default model

        try:
            with open(PROCESS_INFO_FILE, "r") as f:
                data = json.load(f)

            # Try to parse as ProcessInfo with validation
            try:
                # Convert status string to enum if needed
                if "status" in data and isinstance(data["status"], str):
                    try:
                        data["status"] = TrainingStatusEnum(data["status"])
                    except ValueError:
                        logger.warning(f"Invalid status value: {data['status']}")
                        data["status"] = TrainingStatusEnum.UNKNOWN

                return ProcessInfo(**data)
            except Exception as e:
                logger.error(f"Error parsing process info, using defaults: {e}")
                return ProcessInfo()

        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"Error reading process info: {e}")
            return ProcessInfo()


# Create a singleton scheduler instance
scheduler = TrainingScheduler()
